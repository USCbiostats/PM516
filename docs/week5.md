# Week 5 - Monday, September 25

## Statistical Mistakes

&#x27A4; This week we will learn about statistical mistakes to avoid when conducting analyses.

### Before Class

üìñ [Ten Common Statistical Mistakes to Watch Out For When Writing or Reviewing A Manuscript](https://elifesciences.org/articles/48175) <br />

*Recall the common statistical mistakes:*

* Adequate control condition/group.   

  + Does the study have a control condition or group? If not, explain why this is not necessary.

  + Is the control group adequately powered to detect a difference from the treatment group?

  + Are any biases introduced as a result of the assignment to control vs. treatment condition?
  
* Direct comparisons between two effects. 

  + When comparing groups, is just one statistical test used?

  + Which statistical tests are used for the main conclusions in the study?

* Inflating the units of analysis.  

  + What are the observational units on which the analysis was performed?

  + What are the statistical tests actually testing?

  + Are there any ‚Äúclustering‚Äù variables?
  
* Spurious correlations.   

  + Were all variables checked for outliers?
  + Were all analyses examined for possible effect modification?
  + What visual methods were performed in addition to statistical tests?
  
* Use of small samples.   
  + Is the sample size large enough for you to be confident in your conclusions (honestly)?
  + Have you included confidence intervals in addition to point estimates?
  
* Circular analysis.   
  + What analytical decision rules did you make after performing preliminary analyses?
  + How was the selection criteria biased in favor of the hypothesis being tested?
  
* p-hacking.   
  + Do you believe that your study is more exploratory or confirmatory?
  + Did the outcome of interest change during the analysis?
  
* Multiple comparisons.   
  + How many different associations were examined during your analysis?
  + Would you trust your methods if you were reviewing this paper?
  
* Over-interpreting non-significant results.   
  + Was there any point in which a non-significant result was interpreted as a lack of an effect?
  + Are effect sizes reported along with p-values?
  
* Correlation and causation.   
  + Were any conclusions made with regard to causality? What were these and do they make sense?
  + Would it make sense if the outcome of the study actually caused the independent variable?
